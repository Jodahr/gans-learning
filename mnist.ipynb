{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANS - Introduction\n",
    "This notebook is based on the presented ideas from https://www.tensorflow.org/tutorials/generative/dcgan ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import PIL\n",
    "from IPython import display\n",
    "\n",
    "# custom module\n",
    "import utils as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/ec2-user/Programs/gans-learning/utils.py'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(ut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST data\n",
    "Here we load the data and store it as a tensorflow dataset object: https://www.tensorflow.org/api_docs/python/tf/data/Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = data[0]\n",
    "X_test, y_test = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize and create tf datasets\n",
    "X_train_dataset = tf.data.Dataset.from_tensor_slices((np.expand_dims(X_train/255.,-1)))\n",
    "X_test_dataset = tf.data.Dataset.from_tensor_slices((np.expand_dims(X_test/255.,-1)))\n",
    "\n",
    "y_train_dataset = tf.data.Dataset.from_tensor_slices(ut.keras.backend.one_hot(y_train, num_classes=10))\n",
    "y_test_dataset = tf.data.Dataset.from_tensor_slices(y_test)\n",
    "\n",
    "train_dataset = tf.data.Dataset.zip((X_train_dataset, y_train_dataset))\n",
    "test_dataset = tf.data.Dataset.zip((X_test_dataset, y_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n",
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "for i, element in enumerate(X_test_dataset):\n",
    "    print(element.numpy().shape)\n",
    "    if i > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(X_train_dataset)\n",
    "print(next(iterator).numpy().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom CNN\n",
    "Let's build a simple CNN which can classify the images.\n",
    "The goal is not to use all built-in stuff, but to do the learning iteration *by hand*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 28\n",
    "IMG_WIDTH = 28\n",
    "STEPS_PER_EPOCH = np.ceil(len(y_train)/BATCH_SIZE)\n",
    "CHANNELS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn model\n",
    "img_inputs = tf.keras.layers.Input(shape=(IMG_WIDTH, IMG_HEIGHT, CHANNELS))\n",
    "\n",
    "x = layers.Conv2D(filters=32, kernel_size=(4,4), input_shape=(28,28,1),\n",
    "                  activation=\"relu\")(img_inputs)\n",
    "\n",
    "x = layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=(2,2),\n",
    "                 activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(units=10, activation=\"softmax\")(x)\n",
    "\n",
    "disc_model = tf.keras.Model(inputs=img_inputs, outputs=outputs, name='mnist_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mnist_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 25, 25, 32)        544       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 32)        4128      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 11, 11, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                8010      \n",
      "=================================================================\n",
      "Total params: 12,682\n",
      "Trainable params: 12,682\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "disc_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) train simply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_model.compile(loss=ut.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 235 steps\n",
      "Epoch 1/5\n",
      "235/235 [==============================] - 12s 51ms/step - loss: 1.7145 - accuracy: 0.7696\n",
      "Epoch 2/5\n",
      "235/235 [==============================] - 11s 49ms/step - loss: 1.6079 - accuracy: 0.8571\n",
      "Epoch 3/5\n",
      "235/235 [==============================] - 12s 49ms/step - loss: 1.5903 - accuracy: 0.8728\n",
      "Epoch 4/5\n",
      "235/235 [==============================] - 12s 50ms/step - loss: 1.5125 - accuracy: 0.9532\n",
      "Epoch 5/5\n",
      "235/235 [==============================] - 12s 49ms/step - loss: 1.4945 - accuracy: 0.9693\n"
     ]
    }
   ],
   "source": [
    "history = disc_model.fit(train_dataset.batch(BATCH_SIZE),\n",
    "                         epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) train custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_optimizer = tf.keras.optimizers.RMSprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_training = ut.CustomTraining(model=disc_model,\n",
    "                                    optimizer=discriminator_optimizer,\n",
    "                                   loss = ut.losses.CategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "loss:  0.0016097287133595816\n",
      "epoch:  1\n",
      "loss:  0.001589299995506501\n",
      "epoch:  2\n",
      "loss:  0.0015842452862473919\n",
      "epoch:  3\n",
      "loss:  0.0015813136303988863\n",
      "epoch:  4\n",
      "loss:  0.0015793433472678484\n"
     ]
    }
   ],
   "source": [
    "custom_training.fit(X=X_train_dataset,\n",
    "                    y=y_train_dataset,\n",
    "                    each=1, model_params=False, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = custom_training.model.predict(X_test_dataset.batch(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_oh = tf.keras.backend.one_hot(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cast = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.metrics.categorical_accuracy(y_test, y_pred_cast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, 0, 4, 1, 4, 9, 5, 9])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_cast[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, 0, 4, 1, 4, 9, 5, 9], dtype=uint8)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final shape: batch size, rows, cols, channel\n",
    "img_inputs = tf.keras.Input(shape=(28,28,1), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([256, 28, 28, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_inputs.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn model\n",
    "img_inputs = tf.keras.layers.Input(shape=(IMG_WIDTH, IMG_HEIGHT, CHANNELS))\n",
    "\n",
    "x = layers.Conv2D(filters=32, kernel_size=(4,4), input_shape=(28,28,1),\n",
    "                  activation=\"relu\")(img_inputs)\n",
    "\n",
    "x = layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=(2,2),\n",
    "                 activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
    "\n",
    "disc = tf.keras.Model(inputs=img_inputs, outputs=outputs, name='disc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"disc\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 25, 25, 32)        544       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 11, 11, 32)        4128      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 11, 11, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 801       \n",
      "=================================================================\n",
      "Total params: 5,473\n",
      "Trainable params: 5,473\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "disc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Architecture\n",
    "Needs to generate an image out of N random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build discriminator (fake, real); as above but just two classes --> sigmoid\n",
    "# generator: samke architecture as above, but deconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_vector(size, n=1):\n",
    "    return np.array([np.random.uniform(size=size) for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15553601, 0.24044689, 0.57781855, 0.66172882, 0.80398814,\n",
       "        0.98234661, 0.00443632, 0.12775345, 0.32670848, 0.16832662]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_vector(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_inputs = tf.keras.Input(shape=100, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.sqrt(256)\n",
    "12**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_4:0' shape=(1, 100) dtype=float32>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose?version=stable\n",
    "x = layers.Dense(100,input_shape=noise_inputs, activation=\"relu\")(noise_inputs)\n",
    "x = layers.Reshape(target_shape=(10,10,1))(x)\n",
    "x = layers.Conv2DTranspose(filters=5, kernel_size=(3,3), strides=(2,2))(x)\n",
    "x = layers.Conv2DTranspose(filters=5, kernel_size=(3,3), strides=(1,1))(x)\n",
    "outputs = layers.Conv2DTranspose(filters=1, kernel_size=(6,6), strides=(1,1))(x)\n",
    "gen = tf.keras.Model(inputs=noise_inputs, outputs=outputs, name='mnist_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mnist_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(1, 100)]                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (1, 100)                  10100     \n",
      "_________________________________________________________________\n",
      "reshape_7 (Reshape)          (1, 10, 10, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_18 (Conv2DT (1, 21, 21, 5)            50        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_19 (Conv2DT (1, 23, 23, 5)            230       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_20 (Conv2DT (1, 28, 28, 1)            181       \n",
      "=================================================================\n",
      "Total params: 10,561\n",
      "Trainable params: 10,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vector = noise_vector(100)\n",
    "input_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28, 1)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen.predict(input_vector).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = gen.predict(input_vector)[0].reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test, cmap='gray_r')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    \n",
    "    def __init__(self, real):\n",
    "        self.BATCH_SIZE = 64\n",
    "        self.IMG_HEIGHT = 28\n",
    "        self.IMG_WIDTH = 28\n",
    "        self.CHANNELS = 1\n",
    "        \n",
    "        self.discriminator = self._set_discriminator()\n",
    "        self.generator = self._set_generator()\n",
    "        \n",
    "        self.real = real / 255.\n",
    "        \n",
    "        self.discriminator_optimizer = tf.keras.optimizers.Adam(0.0005)\n",
    "        self.generator_optimizer = tf.keras.optimizers.Adam(0.0005)\n",
    "        \n",
    "        \n",
    "    def _set_discriminator(self):\n",
    "        \n",
    "\n",
    "        \n",
    "        # cnn model\n",
    "        img_inputs = tf.keras.layers.Input(shape=(IMG_WIDTH, IMG_HEIGHT, CHANNELS))\n",
    "\n",
    "        x = layers.Conv2D(filters=32, kernel_size=(4,4), input_shape=(28,28,1),\n",
    "                          activation=\"relu\")(img_inputs)\n",
    "\n",
    "        x = layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "        x = layers.Conv2D(filters=32, kernel_size=(2,2),\n",
    "                          activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        x = layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
    "\n",
    "        disc = tf.keras.Model(inputs=img_inputs, outputs=outputs, name='disc')\n",
    "        \n",
    "        return disc\n",
    "    \n",
    "    def _set_generator(self):\n",
    "        \n",
    "        # https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose?version=stable\n",
    "        noise_inputs = tf.keras.Input(shape=100)\n",
    "        x = layers.Dense(100, activation=\"relu\")(noise_inputs)\n",
    "        x = layers.Reshape(target_shape=(10,10,1))(x)\n",
    "        x = layers.Conv2DTranspose(filters=5, kernel_size=(3,3), strides=(2,2))(x)\n",
    "        x = layers.Conv2DTranspose(filters=5, kernel_size=(3,3), strides=(1,1))(x)\n",
    "        outputs = layers.Conv2DTranspose(filters=1, kernel_size=(6,6), strides=(1,1))(x)\n",
    "        gen = tf.keras.Model(inputs=noise_inputs, outputs=outputs, name='mnist_model')   \n",
    "\n",
    "        return gen\n",
    "    \n",
    "    def noise_vector(self, features, size=1):\n",
    "        return np.array([np.random.uniform(size=features) for i in range(size)])\n",
    "    \n",
    "    def show_image(self, img_array):\n",
    "        plt.imshow(img_array, cmap='gray_r')\n",
    "        plt.axis('off')\n",
    "        \n",
    "    def compile_models(self):\n",
    "        self.discriminator.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=self.discriminator_optimizer,\n",
    "              metrics=['accuracy'])\n",
    "        #self.generator.compile()\n",
    "\n",
    "        \n",
    "    def train_discriminator(self, real_img_array, fake_img_array):\n",
    "        \n",
    "        # reshape real\n",
    "        real_img_array = np.expand_dims(real_img_array,-1)\n",
    "\n",
    "        #print(real_img_array.shape)\n",
    "        #print(fake_img_array.shape)\n",
    "        \n",
    "        X = np.concatenate((real_img_array, fake_img_array))\n",
    "        y_real = np.repeat(1, repeats=len(real_img_array))\n",
    "        y_fake = np.repeat(0, repeats=len(fake_img_array))\n",
    "        y = np.concatenate((y_real, y_fake))\n",
    "        \n",
    "        #print(X.shape)\n",
    "        #print(y.shape)\n",
    "        \n",
    "        X_data = tf.data.Dataset.from_tensor_slices(X)\n",
    "        y_data = tf.data.Dataset.from_tensor_slices(y)\n",
    "        \n",
    "        disc = ut.CustomTraining(model=self.discriminator,\n",
    "                                 optimizer=self.discriminator_optimizer,\n",
    "                                loss = ut.losses.BinaryCrossentropy(from_logits=True))\n",
    "        \n",
    "        disc.fit(X_data, y_data, model_params=False, each=1)\n",
    "        \n",
    "        self.discriminator = disc.model\n",
    "    \n",
    "    def generator_loss(self,y_true, y_pred):\n",
    "        ## infinite loss\n",
    "        loss = ut.losses.BinaryCrossentropy(from_logits=True)\n",
    "        return loss\n",
    "    \n",
    "    def freeze_weights(self, model):\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "    \n",
    "    def unfreeze_weights(self, model):\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = True\n",
    "          \n",
    "    def train_generator(self, fake_data):\n",
    "        \n",
    "        # freeze disc\n",
    "        y_true = tf.data.Dataset.from_tensor_slices(np.repeat(1, repeats=len(fake_data)))\n",
    "        X_data = tf.data.Dataset.from_tensor_slices(fake_data)\n",
    "        #X_fake = self.gen.predict(X_data) \n",
    "        y_pred = self.discriminator.predict(X_data.batch(self.BATCH_SIZE))\n",
    "        \n",
    "        print(y_true)\n",
    "        print(y_pred)\n",
    "        \n",
    "        \n",
    "        #am.fit(fake_data, np.repeat(1, repeats=len(fake_data)), validation_split=0.2, batch_size=self.BATCH_SIZE)\n",
    "        #fake_img_array = self.generator.predict(fake_data)\n",
    "        #y_pred_proba = np.squeeze(self.discriminator.predict(fake_img_array))\n",
    "        ##loss = self.generator_loss(y_pred_proba)\n",
    "        #updates_op = self.optimizer.get_updates(params=self.generator.trainable_weights, loss=self.generator_loss(y_pred_proba))\n",
    "        return am\n",
    "    \n",
    "    def fit(self, epochs=10):\n",
    "        for i in range(epochs):\n",
    "            \n",
    "            # train discriminator\n",
    "            fake_data = self.noise_vector(features=100,size=len(self.real))\n",
    "            #fake_data = np.expand_dims(fake_data,-1)\n",
    "            #print(fake_data.shape)\n",
    "            X_fake = self.generator.predict(fake_data)\n",
    "            \n",
    "            #print(self.real.shape)\n",
    "            #print(X_fake.shape)\n",
    "            \n",
    "            #self.train_discriminator(self.real, X_fake)\n",
    "            \n",
    "            \n",
    "            # train generator\n",
    "            self.train_generator(X_fake)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(real=X_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.compile_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gan.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gan.generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (), types: tf.int64>\n",
      "[[0.5005115 ]\n",
      " [0.50809383]\n",
      " [0.50647765]\n",
      " [0.5069373 ]\n",
      " [0.5039633 ]\n",
      " [0.50703275]\n",
      " [0.5128284 ]\n",
      " [0.50865483]\n",
      " [0.50465137]\n",
      " [0.50920355]\n",
      " [0.5056313 ]\n",
      " [0.51247877]\n",
      " [0.50813144]\n",
      " [0.50764084]\n",
      " [0.5067946 ]\n",
      " [0.51481384]\n",
      " [0.5015429 ]\n",
      " [0.507535  ]\n",
      " [0.51407015]\n",
      " [0.5076521 ]\n",
      " [0.5075105 ]\n",
      " [0.50592303]\n",
      " [0.5075422 ]\n",
      " [0.51167506]\n",
      " [0.51001555]\n",
      " [0.51111794]\n",
      " [0.5096908 ]\n",
      " [0.5095634 ]\n",
      " [0.5115201 ]\n",
      " [0.50688684]\n",
      " [0.50825953]\n",
      " [0.5093751 ]\n",
      " [0.512114  ]\n",
      " [0.5104637 ]\n",
      " [0.5106715 ]\n",
      " [0.51082784]\n",
      " [0.5111838 ]\n",
      " [0.50461483]\n",
      " [0.5109496 ]\n",
      " [0.5114009 ]\n",
      " [0.5094931 ]\n",
      " [0.50864047]\n",
      " [0.50988835]\n",
      " [0.50775933]\n",
      " [0.5062943 ]\n",
      " [0.50705886]\n",
      " [0.5068952 ]\n",
      " [0.5088107 ]\n",
      " [0.50963444]\n",
      " [0.51179606]\n",
      " [0.50713176]\n",
      " [0.50471324]\n",
      " [0.50419605]\n",
      " [0.50096667]\n",
      " [0.50991625]\n",
      " [0.50330997]\n",
      " [0.5152278 ]\n",
      " [0.5063228 ]\n",
      " [0.5080695 ]\n",
      " [0.51203626]\n",
      " [0.508572  ]\n",
      " [0.5069909 ]\n",
      " [0.50946903]\n",
      " [0.5073473 ]\n",
      " [0.5070332 ]\n",
      " [0.5068603 ]\n",
      " [0.5056703 ]\n",
      " [0.50925577]\n",
      " [0.50505584]\n",
      " [0.5106749 ]\n",
      " [0.50539774]\n",
      " [0.5067051 ]\n",
      " [0.50793177]\n",
      " [0.5117402 ]\n",
      " [0.50737756]\n",
      " [0.5069301 ]\n",
      " [0.5068687 ]\n",
      " [0.51517546]\n",
      " [0.5122046 ]\n",
      " [0.514444  ]\n",
      " [0.50552636]\n",
      " [0.5032222 ]\n",
      " [0.5094052 ]\n",
      " [0.5063766 ]\n",
      " [0.507803  ]\n",
      " [0.50698185]\n",
      " [0.5177051 ]\n",
      " [0.5095583 ]\n",
      " [0.5074835 ]\n",
      " [0.50823915]\n",
      " [0.5087841 ]\n",
      " [0.5112091 ]\n",
      " [0.50551224]\n",
      " [0.51055396]\n",
      " [0.511321  ]\n",
      " [0.51050305]\n",
      " [0.5112711 ]\n",
      " [0.50867623]\n",
      " [0.50845337]\n",
      " [0.5086671 ]]\n",
      "<TensorSliceDataset shapes: (), types: tf.int64>\n",
      "[[0.51173687]\n",
      " [0.50628257]\n",
      " [0.507753  ]\n",
      " [0.5088701 ]\n",
      " [0.509943  ]\n",
      " [0.504657  ]\n",
      " [0.51482236]\n",
      " [0.5095756 ]\n",
      " [0.5123039 ]\n",
      " [0.5028053 ]\n",
      " [0.5117334 ]\n",
      " [0.5135265 ]\n",
      " [0.5097728 ]\n",
      " [0.5108634 ]\n",
      " [0.50777775]\n",
      " [0.5061349 ]\n",
      " [0.49790514]\n",
      " [0.5073108 ]\n",
      " [0.5071031 ]\n",
      " [0.50514877]\n",
      " [0.5123116 ]\n",
      " [0.5090244 ]\n",
      " [0.51012415]\n",
      " [0.5076488 ]\n",
      " [0.5110425 ]\n",
      " [0.50956744]\n",
      " [0.5098884 ]\n",
      " [0.5090342 ]\n",
      " [0.50517476]\n",
      " [0.50920033]\n",
      " [0.504963  ]\n",
      " [0.508964  ]\n",
      " [0.51136446]\n",
      " [0.50812405]\n",
      " [0.51429546]\n",
      " [0.50872254]\n",
      " [0.5117001 ]\n",
      " [0.5088946 ]\n",
      " [0.5093354 ]\n",
      " [0.5063418 ]\n",
      " [0.5082635 ]\n",
      " [0.5092486 ]\n",
      " [0.50997585]\n",
      " [0.50844365]\n",
      " [0.5077778 ]\n",
      " [0.50936663]\n",
      " [0.50700057]\n",
      " [0.5041395 ]\n",
      " [0.5100414 ]\n",
      " [0.5031113 ]\n",
      " [0.49713427]\n",
      " [0.5063006 ]\n",
      " [0.5087127 ]\n",
      " [0.50675416]\n",
      " [0.5043586 ]\n",
      " [0.5075892 ]\n",
      " [0.5129131 ]\n",
      " [0.5001377 ]\n",
      " [0.5074134 ]\n",
      " [0.51356536]\n",
      " [0.5030341 ]\n",
      " [0.5151191 ]\n",
      " [0.512253  ]\n",
      " [0.50936675]\n",
      " [0.5098196 ]\n",
      " [0.505599  ]\n",
      " [0.50615895]\n",
      " [0.5060779 ]\n",
      " [0.507258  ]\n",
      " [0.50429237]\n",
      " [0.512664  ]\n",
      " [0.5106921 ]\n",
      " [0.5153548 ]\n",
      " [0.5026408 ]\n",
      " [0.51060903]\n",
      " [0.5070345 ]\n",
      " [0.50647146]\n",
      " [0.51220363]\n",
      " [0.50290704]\n",
      " [0.5114603 ]\n",
      " [0.5067507 ]\n",
      " [0.50610656]\n",
      " [0.5080877 ]\n",
      " [0.50350213]\n",
      " [0.50535756]\n",
      " [0.50724566]\n",
      " [0.5029278 ]\n",
      " [0.50727135]\n",
      " [0.50848335]\n",
      " [0.5110864 ]\n",
      " [0.5092094 ]\n",
      " [0.5058995 ]\n",
      " [0.5123603 ]\n",
      " [0.501751  ]\n",
      " [0.5080777 ]\n",
      " [0.509292  ]\n",
      " [0.51079655]\n",
      " [0.502289  ]\n",
      " [0.5099884 ]\n",
      " [0.5131803 ]]\n",
      "<TensorSliceDataset shapes: (), types: tf.int64>\n",
      "[[0.50680625]\n",
      " [0.5095161 ]\n",
      " [0.50755125]\n",
      " [0.5050778 ]\n",
      " [0.511357  ]\n",
      " [0.5107124 ]\n",
      " [0.5078159 ]\n",
      " [0.5138462 ]\n",
      " [0.5069339 ]\n",
      " [0.51421976]\n",
      " [0.50496435]\n",
      " [0.50110763]\n",
      " [0.5110239 ]\n",
      " [0.51168406]\n",
      " [0.5088536 ]\n",
      " [0.50898856]\n",
      " [0.5045514 ]\n",
      " [0.50544024]\n",
      " [0.5013136 ]\n",
      " [0.50579154]\n",
      " [0.5082501 ]\n",
      " [0.51044047]\n",
      " [0.50814456]\n",
      " [0.5079653 ]\n",
      " [0.5083649 ]\n",
      " [0.504868  ]\n",
      " [0.507383  ]\n",
      " [0.50791174]\n",
      " [0.5047944 ]\n",
      " [0.5052796 ]\n",
      " [0.5074435 ]\n",
      " [0.5036324 ]\n",
      " [0.5097254 ]\n",
      " [0.50625724]\n",
      " [0.5127009 ]\n",
      " [0.5088964 ]\n",
      " [0.50668085]\n",
      " [0.5052405 ]\n",
      " [0.5081536 ]\n",
      " [0.5088043 ]\n",
      " [0.5116432 ]\n",
      " [0.50507534]\n",
      " [0.5018029 ]\n",
      " [0.5052465 ]\n",
      " [0.506841  ]\n",
      " [0.5074981 ]\n",
      " [0.50752777]\n",
      " [0.5059314 ]\n",
      " [0.5021781 ]\n",
      " [0.511133  ]\n",
      " [0.50844055]\n",
      " [0.5116555 ]\n",
      " [0.5039818 ]\n",
      " [0.5106111 ]\n",
      " [0.50271255]\n",
      " [0.5099656 ]\n",
      " [0.51804394]\n",
      " [0.5065499 ]\n",
      " [0.51192355]\n",
      " [0.50214624]\n",
      " [0.50411856]\n",
      " [0.50860417]\n",
      " [0.5099356 ]\n",
      " [0.5132851 ]\n",
      " [0.50610644]\n",
      " [0.5089024 ]\n",
      " [0.51053953]\n",
      " [0.5074308 ]\n",
      " [0.5032028 ]\n",
      " [0.5018951 ]\n",
      " [0.50604963]\n",
      " [0.5110223 ]\n",
      " [0.5104566 ]\n",
      " [0.51491106]\n",
      " [0.5064233 ]\n",
      " [0.5105799 ]\n",
      " [0.5072365 ]\n",
      " [0.5121332 ]\n",
      " [0.5071311 ]\n",
      " [0.51101816]\n",
      " [0.5020916 ]\n",
      " [0.5010656 ]\n",
      " [0.51011807]\n",
      " [0.5072964 ]\n",
      " [0.51009774]\n",
      " [0.50730705]\n",
      " [0.5090264 ]\n",
      " [0.50607145]\n",
      " [0.5127621 ]\n",
      " [0.5062244 ]\n",
      " [0.50885916]\n",
      " [0.50617516]\n",
      " [0.5100123 ]\n",
      " [0.50820917]\n",
      " [0.5036274 ]\n",
      " [0.5093263 ]\n",
      " [0.5086003 ]\n",
      " [0.50714874]\n",
      " [0.51002866]\n",
      " [0.5085617 ]]\n",
      "<TensorSliceDataset shapes: (), types: tf.int64>\n",
      "[[0.5039719 ]\n",
      " [0.5075275 ]\n",
      " [0.5026027 ]\n",
      " [0.50405496]\n",
      " [0.50329405]\n",
      " [0.50903934]\n",
      " [0.5035908 ]\n",
      " [0.5040797 ]\n",
      " [0.50535756]\n",
      " [0.5088982 ]\n",
      " [0.50941265]\n",
      " [0.5084052 ]\n",
      " [0.5069035 ]\n",
      " [0.512467  ]\n",
      " [0.5031203 ]\n",
      " [0.50509965]\n",
      " [0.5027437 ]\n",
      " [0.50951046]\n",
      " [0.5106504 ]\n",
      " [0.5107002 ]\n",
      " [0.50556505]\n",
      " [0.5057506 ]\n",
      " [0.51070553]\n",
      " [0.5063894 ]\n",
      " [0.50697374]\n",
      " [0.5075821 ]\n",
      " [0.50541574]\n",
      " [0.5052138 ]\n",
      " [0.5080104 ]\n",
      " [0.5133495 ]\n",
      " [0.5024149 ]\n",
      " [0.5066852 ]\n",
      " [0.5019052 ]\n",
      " [0.5030747 ]\n",
      " [0.50833786]\n",
      " [0.5067942 ]\n",
      " [0.5109221 ]\n",
      " [0.51281136]\n",
      " [0.5088085 ]\n",
      " [0.5032307 ]\n",
      " [0.50447845]\n",
      " [0.5070302 ]\n",
      " [0.5057824 ]\n",
      " [0.50935704]\n",
      " [0.504241  ]\n",
      " [0.50270134]\n",
      " [0.5092381 ]\n",
      " [0.5071696 ]\n",
      " [0.50676006]\n",
      " [0.5035687 ]\n",
      " [0.5041633 ]\n",
      " [0.50753886]\n",
      " [0.5045565 ]\n",
      " [0.50912654]\n",
      " [0.5092082 ]\n",
      " [0.501874  ]\n",
      " [0.5144111 ]\n",
      " [0.5071875 ]\n",
      " [0.51072526]\n",
      " [0.51149154]\n",
      " [0.50576097]\n",
      " [0.5093729 ]\n",
      " [0.5049619 ]\n",
      " [0.49653068]\n",
      " [0.5073163 ]\n",
      " [0.5016578 ]\n",
      " [0.5017897 ]\n",
      " [0.506987  ]\n",
      " [0.5114881 ]\n",
      " [0.51534945]\n",
      " [0.5083196 ]\n",
      " [0.5095273 ]\n",
      " [0.50730497]\n",
      " [0.5070617 ]\n",
      " [0.507085  ]\n",
      " [0.5058534 ]\n",
      " [0.5033819 ]\n",
      " [0.5082477 ]\n",
      " [0.50716835]\n",
      " [0.5097779 ]\n",
      " [0.5036335 ]\n",
      " [0.512067  ]\n",
      " [0.49779794]\n",
      " [0.50483567]\n",
      " [0.50539625]\n",
      " [0.50828964]\n",
      " [0.50912553]\n",
      " [0.5084913 ]\n",
      " [0.5018164 ]\n",
      " [0.5025543 ]\n",
      " [0.5060706 ]\n",
      " [0.50752276]\n",
      " [0.5075824 ]\n",
      " [0.50624907]\n",
      " [0.5046083 ]\n",
      " [0.5075917 ]\n",
      " [0.506987  ]\n",
      " [0.5128426 ]\n",
      " [0.50097454]\n",
      " [0.5065318 ]]\n",
      "<TensorSliceDataset shapes: (), types: tf.int64>\n",
      "[[0.5068756 ]\n",
      " [0.5084052 ]\n",
      " [0.5121654 ]\n",
      " [0.4985969 ]\n",
      " [0.50341463]\n",
      " [0.50974363]\n",
      " [0.5057696 ]\n",
      " [0.50879496]\n",
      " [0.510401  ]\n",
      " [0.51033884]\n",
      " [0.51045066]\n",
      " [0.5110777 ]\n",
      " [0.50626975]\n",
      " [0.513741  ]\n",
      " [0.5090811 ]\n",
      " [0.50325745]\n",
      " [0.5084192 ]\n",
      " [0.5073109 ]\n",
      " [0.511317  ]\n",
      " [0.50483686]\n",
      " [0.51101625]\n",
      " [0.5086668 ]\n",
      " [0.5050761 ]\n",
      " [0.5106184 ]\n",
      " [0.5072539 ]\n",
      " [0.5034293 ]\n",
      " [0.5073495 ]\n",
      " [0.50254   ]\n",
      " [0.51369613]\n",
      " [0.50830764]\n",
      " [0.5068209 ]\n",
      " [0.5120433 ]\n",
      " [0.50552833]\n",
      " [0.51297754]\n",
      " [0.5061398 ]\n",
      " [0.5093192 ]\n",
      " [0.5107334 ]\n",
      " [0.50002635]\n",
      " [0.5060338 ]\n",
      " [0.5068514 ]\n",
      " [0.5054924 ]\n",
      " [0.5044422 ]\n",
      " [0.508347  ]\n",
      " [0.51056445]\n",
      " [0.5060377 ]\n",
      " [0.5103152 ]\n",
      " [0.50372666]\n",
      " [0.51116854]\n",
      " [0.50793445]\n",
      " [0.5056902 ]\n",
      " [0.5146166 ]\n",
      " [0.51094776]\n",
      " [0.50449145]\n",
      " [0.51314676]\n",
      " [0.5035965 ]\n",
      " [0.51153344]\n",
      " [0.5080096 ]\n",
      " [0.5138641 ]\n",
      " [0.5101965 ]\n",
      " [0.5093143 ]\n",
      " [0.5093474 ]\n",
      " [0.49657455]\n",
      " [0.5046401 ]\n",
      " [0.50929   ]\n",
      " [0.50051975]\n",
      " [0.5089328 ]\n",
      " [0.50855464]\n",
      " [0.5061468 ]\n",
      " [0.50671864]\n",
      " [0.5076604 ]\n",
      " [0.51280224]\n",
      " [0.5055707 ]\n",
      " [0.5118161 ]\n",
      " [0.5117787 ]\n",
      " [0.5106864 ]\n",
      " [0.51063836]\n",
      " [0.5016158 ]\n",
      " [0.50629616]\n",
      " [0.50016266]\n",
      " [0.5102508 ]\n",
      " [0.5000431 ]\n",
      " [0.50857496]\n",
      " [0.5042942 ]\n",
      " [0.5064669 ]\n",
      " [0.5025956 ]\n",
      " [0.50968975]\n",
      " [0.5027429 ]\n",
      " [0.5139714 ]\n",
      " [0.51494795]\n",
      " [0.50681674]\n",
      " [0.5062409 ]\n",
      " [0.5099444 ]\n",
      " [0.5086851 ]\n",
      " [0.5087295 ]\n",
      " [0.51478046]\n",
      " [0.5054504 ]\n",
      " [0.50928974]\n",
      " [0.50675434]\n",
      " [0.5098612 ]\n",
      " [0.5077937 ]]\n",
      "<TensorSliceDataset shapes: (), types: tf.int64>\n",
      "[[0.5057258 ]\n",
      " [0.5077905 ]\n",
      " [0.5180607 ]\n",
      " [0.5112029 ]\n",
      " [0.5089315 ]\n",
      " [0.5094907 ]\n",
      " [0.51111346]\n",
      " [0.5056137 ]\n",
      " [0.50931734]\n",
      " [0.51330286]\n",
      " [0.5056336 ]\n",
      " [0.500562  ]\n",
      " [0.502069  ]\n",
      " [0.5141187 ]\n",
      " [0.5055059 ]\n",
      " [0.502106  ]\n",
      " [0.5124612 ]\n",
      " [0.505035  ]\n",
      " [0.51003444]\n",
      " [0.51336306]\n",
      " [0.51048154]\n",
      " [0.50619733]\n",
      " [0.5102324 ]\n",
      " [0.5055657 ]\n",
      " [0.50584763]\n",
      " [0.5048302 ]\n",
      " [0.50545603]\n",
      " [0.5066273 ]\n",
      " [0.5133285 ]\n",
      " [0.5006553 ]\n",
      " [0.5038394 ]\n",
      " [0.50766355]\n",
      " [0.5095694 ]\n",
      " [0.50724703]\n",
      " [0.5091808 ]\n",
      " [0.5069278 ]\n",
      " [0.5077057 ]\n",
      " [0.51033616]\n",
      " [0.51077974]\n",
      " [0.5068113 ]\n",
      " [0.509375  ]\n",
      " [0.5080003 ]\n",
      " [0.50489914]\n",
      " [0.5089632 ]\n",
      " [0.51148665]\n",
      " [0.50542617]\n",
      " [0.51128006]\n",
      " [0.5127189 ]\n",
      " [0.505626  ]\n",
      " [0.5062572 ]\n",
      " [0.50952345]\n",
      " [0.50602424]\n",
      " [0.50938034]\n",
      " [0.50607157]\n",
      " [0.50906324]\n",
      " [0.5107977 ]\n",
      " [0.5050051 ]\n",
      " [0.51212645]\n",
      " [0.51012605]\n",
      " [0.51367533]\n",
      " [0.5107519 ]\n",
      " [0.50415915]\n",
      " [0.5074872 ]\n",
      " [0.5116955 ]\n",
      " [0.51166826]\n",
      " [0.5109056 ]\n",
      " [0.49975693]\n",
      " [0.5087638 ]\n",
      " [0.51401967]\n",
      " [0.5087549 ]\n",
      " [0.51229554]\n",
      " [0.5047606 ]\n",
      " [0.50930876]\n",
      " [0.50231355]\n",
      " [0.5040091 ]\n",
      " [0.50502574]\n",
      " [0.50657475]\n",
      " [0.504793  ]\n",
      " [0.5119741 ]\n",
      " [0.5090309 ]\n",
      " [0.5138204 ]\n",
      " [0.50767916]\n",
      " [0.51190466]\n",
      " [0.5048832 ]\n",
      " [0.51238245]\n",
      " [0.50797933]\n",
      " [0.5112741 ]\n",
      " [0.5041884 ]\n",
      " [0.50579864]\n",
      " [0.50822717]\n",
      " [0.51471704]\n",
      " [0.51187986]\n",
      " [0.50922376]\n",
      " [0.51005346]\n",
      " [0.5132488 ]\n",
      " [0.5104712 ]\n",
      " [0.51211286]\n",
      " [0.50871676]\n",
      " [0.51333505]\n",
      " [0.5072393 ]]\n",
      "<TensorSliceDataset shapes: (), types: tf.int64>\n",
      "[[0.5080658 ]\n",
      " [0.5147733 ]\n",
      " [0.5111867 ]\n",
      " [0.5112224 ]\n",
      " [0.5090079 ]\n",
      " [0.502105  ]\n",
      " [0.50637573]\n",
      " [0.5122825 ]\n",
      " [0.5121887 ]\n",
      " [0.5118869 ]\n",
      " [0.503146  ]\n",
      " [0.5108492 ]\n",
      " [0.50990784]\n",
      " [0.5110134 ]\n",
      " [0.5112761 ]\n",
      " [0.50987315]\n",
      " [0.51060575]\n",
      " [0.5071022 ]\n",
      " [0.50163525]\n",
      " [0.50761974]\n",
      " [0.5094389 ]\n",
      " [0.5090182 ]\n",
      " [0.51775825]\n",
      " [0.50885916]\n",
      " [0.51253945]\n",
      " [0.5069341 ]\n",
      " [0.50793594]\n",
      " [0.50410146]\n",
      " [0.5086785 ]\n",
      " [0.5051197 ]\n",
      " [0.5074819 ]\n",
      " [0.5048987 ]\n",
      " [0.51307654]\n",
      " [0.50932586]\n",
      " [0.5050564 ]\n",
      " [0.51256526]\n",
      " [0.5078694 ]\n",
      " [0.512251  ]\n",
      " [0.5108322 ]\n",
      " [0.50906146]\n",
      " [0.5070496 ]\n",
      " [0.509835  ]\n",
      " [0.50707614]\n",
      " [0.50582546]\n",
      " [0.50616944]\n",
      " [0.508344  ]\n",
      " [0.506883  ]\n",
      " [0.5064919 ]\n",
      " [0.50562316]\n",
      " [0.50893694]\n",
      " [0.5006964 ]\n",
      " [0.506324  ]\n",
      " [0.5099315 ]\n",
      " [0.51027256]\n",
      " [0.51076186]\n",
      " [0.509472  ]\n",
      " [0.5056962 ]\n",
      " [0.5153388 ]\n",
      " [0.51068574]\n",
      " [0.50601906]\n",
      " [0.50646275]\n",
      " [0.51013064]\n",
      " [0.51139283]\n",
      " [0.5151407 ]\n",
      " [0.50507575]\n",
      " [0.511167  ]\n",
      " [0.5070927 ]\n",
      " [0.50887215]\n",
      " [0.51229215]\n",
      " [0.5050623 ]\n",
      " [0.5121753 ]\n",
      " [0.5099623 ]\n",
      " [0.50428736]\n",
      " [0.50408113]\n",
      " [0.5034239 ]\n",
      " [0.50771505]\n",
      " [0.5058339 ]\n",
      " [0.5097674 ]\n",
      " [0.51200134]\n",
      " [0.504617  ]\n",
      " [0.502557  ]\n",
      " [0.50934154]\n",
      " [0.510663  ]\n",
      " [0.51536465]\n",
      " [0.5047794 ]\n",
      " [0.50337094]\n",
      " [0.5048348 ]\n",
      " [0.5029522 ]\n",
      " [0.5123788 ]\n",
      " [0.50848734]\n",
      " [0.50340307]\n",
      " [0.5094304 ]\n",
      " [0.5078564 ]\n",
      " [0.51168114]\n",
      " [0.5099943 ]\n",
      " [0.50985193]\n",
      " [0.51103586]\n",
      " [0.50841254]\n",
      " [0.5085651 ]\n",
      " [0.5157163 ]]\n",
      "<TensorSliceDataset shapes: (), types: tf.int64>\n",
      "[[0.5093544 ]\n",
      " [0.51228803]\n",
      " [0.5112173 ]\n",
      " [0.5116606 ]\n",
      " [0.5057193 ]\n",
      " [0.51007724]\n",
      " [0.5037798 ]\n",
      " [0.5066065 ]\n",
      " [0.50647694]\n",
      " [0.50758654]\n",
      " [0.51152396]\n",
      " [0.50689805]\n",
      " [0.50693154]\n",
      " [0.50742626]\n",
      " [0.51219326]\n",
      " [0.50542027]\n",
      " [0.510454  ]\n",
      " [0.5091747 ]\n",
      " [0.50503373]\n",
      " [0.5120622 ]\n",
      " [0.51028323]\n",
      " [0.5090039 ]\n",
      " [0.50341076]\n",
      " [0.5084915 ]\n",
      " [0.5082705 ]\n",
      " [0.5041181 ]\n",
      " [0.5106504 ]\n",
      " [0.50908476]\n",
      " [0.5076746 ]\n",
      " [0.5103026 ]\n",
      " [0.51047605]\n",
      " [0.5159507 ]\n",
      " [0.5020985 ]\n",
      " [0.5029747 ]\n",
      " [0.5200007 ]\n",
      " [0.5063008 ]\n",
      " [0.5055658 ]\n",
      " [0.50730413]\n",
      " [0.5101913 ]\n",
      " [0.50548553]\n",
      " [0.5056741 ]\n",
      " [0.4998817 ]\n",
      " [0.5055027 ]\n",
      " [0.5048876 ]\n",
      " [0.5095904 ]\n",
      " [0.5071188 ]\n",
      " [0.5104313 ]\n",
      " [0.51115936]\n",
      " [0.50627506]\n",
      " [0.5055766 ]\n",
      " [0.50723857]\n",
      " [0.5043079 ]\n",
      " [0.5095549 ]\n",
      " [0.5063405 ]\n",
      " [0.5105897 ]\n",
      " [0.50135493]\n",
      " [0.51040024]\n",
      " [0.50079495]\n",
      " [0.507343  ]\n",
      " [0.5099456 ]\n",
      " [0.5122196 ]\n",
      " [0.50568014]\n",
      " [0.51018506]\n",
      " [0.5096281 ]\n",
      " [0.5101827 ]\n",
      " [0.5069222 ]\n",
      " [0.507783  ]\n",
      " [0.50839686]\n",
      " [0.50605285]\n",
      " [0.50532174]\n",
      " [0.5060294 ]\n",
      " [0.50772774]\n",
      " [0.5099619 ]\n",
      " [0.50760907]\n",
      " [0.5069526 ]\n",
      " [0.50496733]\n",
      " [0.5096404 ]\n",
      " [0.503426  ]\n",
      " [0.50772643]\n",
      " [0.5049981 ]\n",
      " [0.50903463]\n",
      " [0.5065989 ]\n",
      " [0.5127952 ]\n",
      " [0.5107818 ]\n",
      " [0.5073215 ]\n",
      " [0.50931627]\n",
      " [0.51076114]\n",
      " [0.50617576]\n",
      " [0.5072605 ]\n",
      " [0.50040895]\n",
      " [0.5051523 ]\n",
      " [0.5021256 ]\n",
      " [0.51236445]\n",
      " [0.5112704 ]\n",
      " [0.5017936 ]\n",
      " [0.5049788 ]\n",
      " [0.5049875 ]\n",
      " [0.51147425]\n",
      " [0.50858015]\n",
      " [0.5113366 ]]\n",
      "<TensorSliceDataset shapes: (), types: tf.int64>\n",
      "[[0.5120192 ]\n",
      " [0.5080606 ]\n",
      " [0.5048876 ]\n",
      " [0.50487435]\n",
      " [0.5052002 ]\n",
      " [0.5085261 ]\n",
      " [0.50779635]\n",
      " [0.5041424 ]\n",
      " [0.5130976 ]\n",
      " [0.50885826]\n",
      " [0.5074665 ]\n",
      " [0.51231974]\n",
      " [0.5190071 ]\n",
      " [0.50245816]\n",
      " [0.50699514]\n",
      " [0.50780445]\n",
      " [0.50815266]\n",
      " [0.5112362 ]\n",
      " [0.5126749 ]\n",
      " [0.5053655 ]\n",
      " [0.51211405]\n",
      " [0.50706655]\n",
      " [0.51417077]\n",
      " [0.5149985 ]\n",
      " [0.50392663]\n",
      " [0.5063089 ]\n",
      " [0.51294005]\n",
      " [0.5071328 ]\n",
      " [0.50825584]\n",
      " [0.50731474]\n",
      " [0.5071806 ]\n",
      " [0.50314015]\n",
      " [0.51096314]\n",
      " [0.5025628 ]\n",
      " [0.5071256 ]\n",
      " [0.5078113 ]\n",
      " [0.5113908 ]\n",
      " [0.50424   ]\n",
      " [0.5097692 ]\n",
      " [0.51124495]\n",
      " [0.50046974]\n",
      " [0.5100811 ]\n",
      " [0.50895023]\n",
      " [0.5090437 ]\n",
      " [0.5104087 ]\n",
      " [0.5071717 ]\n",
      " [0.50650036]\n",
      " [0.50503105]\n",
      " [0.51221085]\n",
      " [0.5091036 ]\n",
      " [0.5071238 ]\n",
      " [0.51170236]\n",
      " [0.5093241 ]\n",
      " [0.50837064]\n",
      " [0.50524646]\n",
      " [0.51144725]\n",
      " [0.5031947 ]\n",
      " [0.50703025]\n",
      " [0.5089543 ]\n",
      " [0.5093714 ]\n",
      " [0.504444  ]\n",
      " [0.51697147]\n",
      " [0.51141125]\n",
      " [0.5114673 ]\n",
      " [0.5076304 ]\n",
      " [0.5130304 ]\n",
      " [0.5095615 ]\n",
      " [0.5053571 ]\n",
      " [0.5003978 ]\n",
      " [0.5114806 ]\n",
      " [0.5063508 ]\n",
      " [0.50338477]\n",
      " [0.5055501 ]\n",
      " [0.50607425]\n",
      " [0.51118934]\n",
      " [0.50546914]\n",
      " [0.51147056]\n",
      " [0.50487906]\n",
      " [0.5120396 ]\n",
      " [0.5050068 ]\n",
      " [0.510777  ]\n",
      " [0.51211965]\n",
      " [0.5086505 ]\n",
      " [0.51038   ]\n",
      " [0.5013019 ]\n",
      " [0.4981006 ]\n",
      " [0.50995106]\n",
      " [0.50781626]\n",
      " [0.50719565]\n",
      " [0.5090851 ]\n",
      " [0.5085082 ]\n",
      " [0.5134395 ]\n",
      " [0.5071104 ]\n",
      " [0.5127676 ]\n",
      " [0.5109668 ]\n",
      " [0.5065563 ]\n",
      " [0.5076048 ]\n",
      " [0.50486964]\n",
      " [0.5065251 ]\n",
      " [0.50571865]]\n",
      "<TensorSliceDataset shapes: (), types: tf.int64>\n",
      "[[0.5103866 ]\n",
      " [0.5115252 ]\n",
      " [0.5179771 ]\n",
      " [0.50483817]\n",
      " [0.5081168 ]\n",
      " [0.5076908 ]\n",
      " [0.5029662 ]\n",
      " [0.51619977]\n",
      " [0.50301266]\n",
      " [0.50729287]\n",
      " [0.51002914]\n",
      " [0.5120614 ]\n",
      " [0.50204116]\n",
      " [0.5025776 ]\n",
      " [0.5046704 ]\n",
      " [0.5065706 ]\n",
      " [0.50709045]\n",
      " [0.5042598 ]\n",
      " [0.5094006 ]\n",
      " [0.5089918 ]\n",
      " [0.5048462 ]\n",
      " [0.5033466 ]\n",
      " [0.5094989 ]\n",
      " [0.5092595 ]\n",
      " [0.5096628 ]\n",
      " [0.50546134]\n",
      " [0.5106481 ]\n",
      " [0.50676626]\n",
      " [0.5131715 ]\n",
      " [0.5106911 ]\n",
      " [0.5062652 ]\n",
      " [0.5104477 ]\n",
      " [0.5052105 ]\n",
      " [0.5085574 ]\n",
      " [0.50958425]\n",
      " [0.5160873 ]\n",
      " [0.50632197]\n",
      " [0.51096135]\n",
      " [0.50389826]\n",
      " [0.51088876]\n",
      " [0.50719464]\n",
      " [0.50426424]\n",
      " [0.5073944 ]\n",
      " [0.50597507]\n",
      " [0.5014904 ]\n",
      " [0.5090404 ]\n",
      " [0.5034999 ]\n",
      " [0.51033574]\n",
      " [0.5075482 ]\n",
      " [0.5068308 ]\n",
      " [0.50245464]\n",
      " [0.5133729 ]\n",
      " [0.50888795]\n",
      " [0.5081401 ]\n",
      " [0.5012783 ]\n",
      " [0.5028083 ]\n",
      " [0.51069087]\n",
      " [0.5121212 ]\n",
      " [0.5116224 ]\n",
      " [0.5055584 ]\n",
      " [0.50435627]\n",
      " [0.5082293 ]\n",
      " [0.5089844 ]\n",
      " [0.5084713 ]\n",
      " [0.50574875]\n",
      " [0.5076895 ]\n",
      " [0.51165885]\n",
      " [0.50716645]\n",
      " [0.50952435]\n",
      " [0.5065615 ]\n",
      " [0.50978667]\n",
      " [0.50451624]\n",
      " [0.5085566 ]\n",
      " [0.50923705]\n",
      " [0.5056189 ]\n",
      " [0.51564544]\n",
      " [0.509474  ]\n",
      " [0.5089229 ]\n",
      " [0.50665796]\n",
      " [0.51371276]\n",
      " [0.5051275 ]\n",
      " [0.5135882 ]\n",
      " [0.50575614]\n",
      " [0.50460106]\n",
      " [0.5027881 ]\n",
      " [0.5067386 ]\n",
      " [0.50500166]\n",
      " [0.5059617 ]\n",
      " [0.5116974 ]\n",
      " [0.50400585]\n",
      " [0.50724864]\n",
      " [0.50529575]\n",
      " [0.50575864]\n",
      " [0.5074599 ]\n",
      " [0.5057566 ]\n",
      " [0.50895184]\n",
      " [0.50267375]\n",
      " [0.50040656]\n",
      " [0.50524926]\n",
      " [0.51199555]]\n"
     ]
    }
   ],
   "source": [
    "gan.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0x7f558e12a3d0>"
      ]
     },
     "execution_count": 918,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan.generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=-4.8158912173037445>"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan.generator_loss(0,np.array([0.1,0.1,0.9,0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.compile_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gan.generator.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12800 samples, validate on 3200 samples\n",
      "12800/12800 [==============================] - 6s 443us/sample - loss: 2.1148 - accuracy: 0.0025 - val_loss: 2.0796 - val_accuracy: 3.1250e-04\n",
      "Model: \"sequential_163\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mnist_generator (Model)      (32, 28, 28, 1)           461       \n",
      "_________________________________________________________________\n",
      "mnist_discriminator (Model)  (32, 1)                   645       \n",
      "=================================================================\n",
      "Total params: 1,106\n",
      "Trainable params: 461\n",
      "Non-trainable params: 645\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.train_generator().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mnist_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_165 (InputLayer)       [(32, 100, 1)]            0         \n",
      "_________________________________________________________________\n",
      "reshape_124 (Reshape)        (32, 10, 10, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_253 (Conv2D (32, 21, 21, 5)           50        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_254 (Conv2D (32, 23, 23, 5)           230       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_255 (Conv2D (32, 28, 28, 1)           181       \n",
      "=================================================================\n",
      "Total params: 461\n",
      "Trainable params: 461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.compile_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_discriminator() missing 2 required positional arguments: 'real_img_array' and 'fake_img_array'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-481-96ed32316f84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: train_discriminator() missing 2 required positional arguments: 'real_img_array' and 'fake_img_array'"
     ]
    }
   ],
   "source": [
    "gan.train_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fake = gan.noise_vector(features=100, size=12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 100)"
      ]
     },
     "execution_count": 749,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 28, 28, 1)"
      ]
     },
     "execution_count": 870,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake = gan.generator.predict(np.expand_dims(X_fake,-1))\n",
    "fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gan.train_generator(X_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 872,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(fake[0]).shape\n",
    "img_array = np.squeeze(fake[10])\n",
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gan.show_image(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99975353],\n",
       "       [0.9995646 ],\n",
       "       [0.99963963],\n",
       "       ...,\n",
       "       [0.99988246],\n",
       "       [0.9997265 ],\n",
       "       [0.99931383]], dtype=float32)"
      ]
     },
     "execution_count": 874,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan.discriminator.predict(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_190:0\", shape=(32, 28, 28, 1), dtype=float32) for input (32, 28, 28, 1), but it was re-called on a Tensor with incompatible shape (60000, 28, 28, 1).\n",
      "WARNING:tensorflow:Layer conv2d_156 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(60000, 1), dtype=float32, numpy=\n",
       "array([[0.9999076 ],\n",
       "       [0.99998903],\n",
       "       [0.8505106 ],\n",
       "       ...,\n",
       "       [0.9990171 ],\n",
       "       [0.99846363],\n",
       "       [0.9983368 ]], dtype=float32)>"
      ]
     },
     "execution_count": 875,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan.discriminator(np.expand_dims(gan.real,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(12000, 28, 28, 1)\n",
      "(72000, 28, 28, 1)\n",
      "(72000,)\n",
      "Train on 57600 samples, validate on 14400 samples\n",
      "57600/57600 [==============================] - 17s 302us/sample - loss: 5.5199e-08 - accuracy: 1.0000 - val_loss: 0.6836 - val_accuracy: 0.1667\n"
     ]
    }
   ],
   "source": [
    "gan.train_discriminator(gan.real, fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
