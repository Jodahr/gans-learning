{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = data[0]\n",
    "X_test, y_test = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28 at 0x7F559CE05490>,\n",
       " <PIL.Image.Image image mode=L size=28x28 at 0x7F559CE053D0>)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(X_train[1]), Image.fromarray(X_train[2]), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[0], cmap='gray_r', vmin=0, vmax=255)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count_train = len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 28\n",
    "IMG_WIDTH = 28\n",
    "STEPS_PER_EPOCH = np.ceil(image_count_train/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_s = X_train / 255.\n",
    "X_test_s = X_test / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final shape: batch size, rows, cols, channel\n",
    "img_inputs = tf.keras.Input(shape=(28,28,1), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 28, 28, 1])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_inputs.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Conv2D(filters=32, kernel_size=(4,4), input_shape=(28,28,1),\n",
    "                  activation=\"relu\")(img_inputs)\n",
    "x = layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=(2,2),\n",
    "                 activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(units=10, activation=\"softmax\")(x)\n",
    "\n",
    "disc_model = tf.keras.Model(inputs=img_inputs, outputs=outputs, name='mnist_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mnist_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_33 (InputLayer)        [(32, 28, 28, 1)]         0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (32, 25, 25, 32)          544       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (32, 12, 12, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (32, 11, 11, 32)          4128      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (32, 11, 11, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (32, 5, 5, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (32, 800)                 0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (32, 10)                  8010      \n",
      "=================================================================\n",
      "Total params: 12,682\n",
      "Trainable params: 12,682\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "disc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# channel is missing\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add channel\n",
    "X = np.expand_dims(X_train_s,-1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "48000/48000 [==============================] - 15s 308us/sample - loss: 0.2420 - accuracy: 0.9269 - val_loss: 0.1025 - val_accuracy: 0.9695\n",
      "Epoch 2/5\n",
      "48000/48000 [==============================] - 14s 297us/sample - loss: 0.0855 - accuracy: 0.9739 - val_loss: 0.0794 - val_accuracy: 0.9777\n",
      "Epoch 3/5\n",
      "48000/48000 [==============================] - 14s 299us/sample - loss: 0.0658 - accuracy: 0.9795 - val_loss: 0.0608 - val_accuracy: 0.9821\n",
      "Epoch 4/5\n",
      "48000/48000 [==============================] - 14s 298us/sample - loss: 0.0559 - accuracy: 0.9830 - val_loss: 0.0567 - val_accuracy: 0.9845\n",
      "Epoch 5/5\n",
      "48000/48000 [==============================] - 14s 297us/sample - loss: 0.0495 - accuracy: 0.9847 - val_loss: 0.0473 - val_accuracy: 0.9867\n"
     ]
    }
   ],
   "source": [
    "disc_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "history = disc_model.fit(X, y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=5,\n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.expand_dims(X_test,-1))[0].argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### expand dim\n",
    "Added a feature dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,1,1,1,1])\n",
    "print(a)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_exp = np.expand_dims(a,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "print(a_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_exp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Architecture\n",
    "Needs to generate an image out of N random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build discriminator (fake, real); as above but just two classes --> sigmoid\n",
    "# generator: samke architecture as above, but deconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_vector(size, n=1):\n",
    "    return np.array([np.random.uniform(size=size) for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79170708, 0.46052481, 0.65414726, 0.19719191, 0.9193853 ,\n",
       "        0.60837298, 0.62500247, 0.2427843 , 0.08571641, 0.62737638]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_vector(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_inputs = tf.keras.Input(shape=100, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.sqrt(256)\n",
    "12**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_8:0' shape=(1, 100) dtype=float32>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose?version=stable\n",
    "#x = layers.Dense(100,input_shape=noise_inputs, activation=\"relu\")(noise_inputs)\n",
    "x = layers.Reshape(target_shape=(10,10,1), input_shape=noise_inputs)(noise_inputs)\n",
    "x = layers.Conv2DTranspose(filters=5, kernel_size=(3,3), strides=(2,2))(x)\n",
    "x = layers.Conv2DTranspose(filters=5, kernel_size=(3,3), strides=(1,1))(x)\n",
    "outputs = layers.Conv2DTranspose(filters=1, kernel_size=(6,6), strides=(1,1))(x)\n",
    "model = tf.keras.Model(inputs=noise_inputs, outputs=outputs, name='mnist_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mnist_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(1, 100)]                0         \n",
      "_________________________________________________________________\n",
      "reshape_107 (Reshape)        (1, 10, 10, 1)            0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_223 (Conv2D (1, 21, 21, 5)            50        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_224 (Conv2D (1, 23, 23, 5)            230       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_225 (Conv2D (1, 28, 28, 1)            181       \n",
      "=================================================================\n",
      "Total params: 461\n",
      "Trainable params: 461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vector = noise_vector(100)\n",
    "input_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28, 1)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(input_vector).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = model.predict(input_vector)[0].reshape(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(test, cmap='gray_r')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    \n",
    "    def __init__(self, real):\n",
    "        self.BATCH_SIZE = 32\n",
    "        self.IMG_HEIGHT = 28\n",
    "        self.IMG_WIDTH = 28\n",
    "        self.CHANNELS = 1\n",
    "        self.STEPS_PER_EPOCH = np.ceil(image_count_train/self.BATCH_SIZE)\n",
    "        \n",
    "        self.discriminator = self._set_discriminator()\n",
    "        self.generator = self._set_generator()\n",
    "        \n",
    "        self.real = real / 255.\n",
    "        \n",
    "        self.discriminator_optimizer = tf.keras.optimizers.Adam(0.0005)\n",
    "        self.generator_optimizer = tf.keras.optimizers.Adam(0.0005)\n",
    "        \n",
    "        \n",
    "    def _set_discriminator(self):\n",
    "        \n",
    "        # input\n",
    "        img_inputs = tf.keras.Input(shape=(28,28,1), batch_size=self.BATCH_SIZE)\n",
    "        \n",
    "        # model architecture\n",
    "        x = layers.Conv2D(filters=4, kernel_size=(4,4), input_shape=(self.IMG_HEIGHT,self.IMG_WIDTH,self.CHANNELS),\n",
    "                  activation=\"relu\")(img_inputs)\n",
    "        x = layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "        \n",
    "        #x = layers.Conv2D(filters=32, kernel_size=(2,2),\n",
    "        #                  activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        #x = layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "        \n",
    "        # flatten\n",
    "        x = layers.Flatten()(x)\n",
    "        outputs = layers.Dense(units=1, activation=\"sigmoid\")(x)\n",
    "     \n",
    "        return tf.keras.Model(inputs=img_inputs, outputs=outputs, name='mnist_discriminator')\n",
    "    \n",
    "    def _set_generator(self):\n",
    "        \n",
    "        noise_inputs = tf.keras.Input(shape=(100,1), batch_size=self.BATCH_SIZE)\n",
    "        x = layers.Reshape(target_shape=(10,10,1), input_shape=noise_inputs)(noise_inputs)\n",
    "        x = layers.Conv2DTranspose(filters=10, kernel_size=(3,3), strides=(2,2))(x)\n",
    "        x = layers.Conv2DTranspose(filters=10, kernel_size=(3,3), strides=(1,1))(x)\n",
    "        outputs = layers.Conv2DTranspose(filters=1, kernel_size=(6,6), strides=(1,1))(x)\n",
    "\n",
    "        #x = layers.Dense(10,input_shape=noise_inputs, activation=\"relu\")(noise_inputs)\n",
    "        #outputs = layers.Reshape(target_shape=(28,28,1), input_shape=noise_inputs)(x)\n",
    "        #x = layers.Conv2DTranspose(filters=1, kernel_size=(3,3), strides=(2,2))(x)\n",
    "        #x = layers.Conv2DTranspose(filters=1, kernel_size=(3,3), strides=(1,1))(x)\n",
    "        #outputs = layers.Conv2DTranspose(filters=1, kernel_size=(6,6), strides=(1,1))(x)\n",
    "        return tf.keras.Model(inputs=noise_inputs, outputs=outputs, name='mnist_generator')\n",
    "    \n",
    "    def noise_vector(self, features, size=1):\n",
    "        return np.array([np.random.uniform(size=features) for i in range(size)])\n",
    "    \n",
    "    def show_image(self, img_array):\n",
    "        plt.imshow(img_array, cmap='gray_r')\n",
    "        plt.axis('off')\n",
    "        \n",
    "    def compile_models(self):\n",
    "        self.discriminator.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=self.discriminator_optimizer,\n",
    "              metrics=['accuracy'])\n",
    "        #self.generator.compile()\n",
    "\n",
    "        \n",
    "    def train_discriminator(self, real_img_array, fake_img_array):\n",
    "        self.unfreeze_weights(self.discriminator)\n",
    "        #self.compile_models()\n",
    "        \n",
    "        \n",
    "        # reshape real\n",
    "        real_img_array = np.expand_dims(real_img_array,-1)\n",
    "\n",
    "        print(real_img_array.shape)\n",
    "        print(fake_img_array.shape)\n",
    "        \n",
    "        X = np.concatenate((real_img_array, fake_img_array))\n",
    "        y_real = np.repeat(1, repeats=len(real_img_array))\n",
    "        y_fake = np.repeat(0, repeats=len(fake_img_array))\n",
    "        y = np.concatenate((y_real, y_fake))\n",
    "        \n",
    "        print(X.shape)\n",
    "        print(y.shape)\n",
    "        \n",
    "        history = self.discriminator.fit(X, y,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=1,\n",
    "                    validation_split=0.2)\n",
    "    \n",
    "    def generator_loss(self,y_true, y_pred):\n",
    "        ## infinite loss\n",
    "        loss = K.sum(K.log(1-y_pred))\n",
    "        return loss\n",
    "    \n",
    "    def freeze_weights(self, model):\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "    \n",
    "    def unfreeze_weights(self, model):\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = True\n",
    "          \n",
    "    def train_generator(self, fake_data):\n",
    "        \n",
    "        # freeze disc\n",
    "        self.freeze_weights(self.discriminator)\n",
    "        \n",
    "        am = tf.keras.Sequential()\n",
    "        am.add(self.generator)\n",
    "        am.add(self.discriminator)\n",
    "        am.compile(loss=\"binary_crossentropy\", optimizer=self.generator_optimizer, metrics=['accuracy'])\n",
    "        \n",
    "        #fake_data = self.noise_vector(features=100,size=1600)\n",
    "        #fake_data = np.expand_dims(fake_data,-1)\n",
    "        #for i in range(5):\n",
    "        am.fit(fake_data, np.repeat(1, repeats=len(fake_data)), validation_split=0.2)\n",
    "        #fake_img_array = self.generator.predict(fake_data)\n",
    "        #y_pred_proba = np.squeeze(self.discriminator.predict(fake_img_array))\n",
    "        ##loss = self.generator_loss(y_pred_proba)\n",
    "        #updates_op = self.optimizer.get_updates(params=self.generator.trainable_weights, loss=self.generator_loss(y_pred_proba))\n",
    "        return am\n",
    "    \n",
    "    def fit(self, epochs=5):\n",
    "        for i in range(epochs):\n",
    "            \n",
    "            # train discriminator\n",
    "            fake_data = self.noise_vector(features=100,size=1600)\n",
    "            fake_data = np.expand_dims(fake_data,-1)\n",
    "            X_fake = gan.generator.predict(fake_data)\n",
    "            self.train_discriminator(self.real[:1600], X_fake)\n",
    "            \n",
    "            # train generator\n",
    "            self.train_generator(fake_data)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(real=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.compile_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mnist_discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_206 (InputLayer)       [(32, 28, 28, 1)]         0         \n",
      "_________________________________________________________________\n",
      "conv2d_164 (Conv2D)          (32, 25, 25, 4)           68        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_163 (MaxPoolin (32, 12, 12, 4)           0         \n",
      "_________________________________________________________________\n",
      "dropout_99 (Dropout)         (32, 12, 12, 4)           0         \n",
      "_________________________________________________________________\n",
      "flatten_104 (Flatten)        (32, 576)                 0         \n",
      "_________________________________________________________________\n",
      "dense_188 (Dense)            (32, 1)                   577       \n",
      "=================================================================\n",
      "Total params: 645\n",
      "Trainable params: 645\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mnist_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_207 (InputLayer)       [(32, 100, 1)]            0         \n",
      "_________________________________________________________________\n",
      "reshape_145 (Reshape)        (32, 10, 10, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_316 (Conv2D (32, 21, 21, 10)          100       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_317 (Conv2D (32, 23, 23, 10)          910       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_318 (Conv2D (32, 28, 28, 1)           361       \n",
      "=================================================================\n",
      "Total params: 1,371\n",
      "Trainable params: 1,371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 28, 28, 1)\n",
      "(1600, 28, 28, 1)\n",
      "(3200, 28, 28, 1)\n",
      "(3200,)\n",
      "Train on 2560 samples, validate on 640 samples\n",
      "2560/2560 [==============================] - 1s 351us/sample - loss: 0.4994 - accuracy: 0.9094 - val_loss: 0.3941 - val_accuracy: 1.0000\n",
      "Train on 1280 samples, validate on 320 samples\n",
      "1280/1280 [==============================] - 1s 949us/sample - loss: 0.4381 - accuracy: 0.9141 - val_loss: 0.2351 - val_accuracy: 1.0000\n",
      "(1600, 28, 28, 1)\n",
      "(1600, 28, 28, 1)\n",
      "(3200, 28, 28, 1)\n",
      "(3200,)\n",
      "Train on 2560 samples, validate on 640 samples\n",
      "2560/2560 [==============================] - 1s 202us/sample - loss: 0.3507 - accuracy: 0.8898 - val_loss: 0.1343 - val_accuracy: 1.0000\n",
      "Train on 1280 samples, validate on 320 samples\n",
      "1280/1280 [==============================] - 1s 763us/sample - loss: 1.1571 - accuracy: 0.0820 - val_loss: 0.7357 - val_accuracy: 0.2281\n",
      "(1600, 28, 28, 1)\n",
      "(1600, 28, 28, 1)\n",
      "(3200, 28, 28, 1)\n",
      "(3200,)\n",
      "Train on 2560 samples, validate on 640 samples\n",
      "2560/2560 [==============================] - 1s 200us/sample - loss: 0.3242 - accuracy: 0.9191 - val_loss: 0.4549 - val_accuracy: 1.0000\n",
      "Train on 1280 samples, validate on 320 samples\n",
      "1184/1280 [==========================>...] - ETA: 0s - loss: 0.8138 - accuracy: 0.2221"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-924-2e866bbc1d6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-919-8ad600a0309e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;31m# train generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-919-8ad600a0309e>\u001b[0m in \u001b[0;36mtrain_generator\u001b[0;34m(self, fake_data)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;31m#fake_data = np.expand_dims(fake_data,-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m#for i in range(5):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;31m#fake_img_array = self.generator.predict(fake_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m#y_pred_proba = np.squeeze(self.discriminator.predict(fake_img_array))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gan.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0x7f558e12a3d0>"
      ]
     },
     "execution_count": 918,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan.generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=-4.8158912173037445>"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan.generator_loss(0,np.array([0.1,0.1,0.9,0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.compile_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gan.generator.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12800 samples, validate on 3200 samples\n",
      "12800/12800 [==============================] - 6s 443us/sample - loss: 2.1148 - accuracy: 0.0025 - val_loss: 2.0796 - val_accuracy: 3.1250e-04\n",
      "Model: \"sequential_163\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mnist_generator (Model)      (32, 28, 28, 1)           461       \n",
      "_________________________________________________________________\n",
      "mnist_discriminator (Model)  (32, 1)                   645       \n",
      "=================================================================\n",
      "Total params: 1,106\n",
      "Trainable params: 461\n",
      "Non-trainable params: 645\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.train_generator().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mnist_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_165 (InputLayer)       [(32, 100, 1)]            0         \n",
      "_________________________________________________________________\n",
      "reshape_124 (Reshape)        (32, 10, 10, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_253 (Conv2D (32, 21, 21, 5)           50        \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_254 (Conv2D (32, 23, 23, 5)           230       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_255 (Conv2D (32, 28, 28, 1)           181       \n",
      "=================================================================\n",
      "Total params: 461\n",
      "Trainable params: 461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.compile_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_discriminator() missing 2 required positional arguments: 'real_img_array' and 'fake_img_array'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-481-96ed32316f84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: train_discriminator() missing 2 required positional arguments: 'real_img_array' and 'fake_img_array'"
     ]
    }
   ],
   "source": [
    "gan.train_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fake = gan.noise_vector(features=100, size=12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 100)"
      ]
     },
     "execution_count": 749,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 28, 28, 1)"
      ]
     },
     "execution_count": 870,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake = gan.generator.predict(np.expand_dims(X_fake,-1))\n",
    "fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gan.train_generator(X_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 872,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(fake[0]).shape\n",
    "img_array = np.squeeze(fake[10])\n",
    "img_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gan.show_image(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99975353],\n",
       "       [0.9995646 ],\n",
       "       [0.99963963],\n",
       "       ...,\n",
       "       [0.99988246],\n",
       "       [0.9997265 ],\n",
       "       [0.99931383]], dtype=float32)"
      ]
     },
     "execution_count": 874,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan.discriminator.predict(fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape Tensor(\"input_190:0\", shape=(32, 28, 28, 1), dtype=float32) for input (32, 28, 28, 1), but it was re-called on a Tensor with incompatible shape (60000, 28, 28, 1).\n",
      "WARNING:tensorflow:Layer conv2d_156 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(60000, 1), dtype=float32, numpy=\n",
       "array([[0.9999076 ],\n",
       "       [0.99998903],\n",
       "       [0.8505106 ],\n",
       "       ...,\n",
       "       [0.9990171 ],\n",
       "       [0.99846363],\n",
       "       [0.9983368 ]], dtype=float32)>"
      ]
     },
     "execution_count": 875,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gan.discriminator(np.expand_dims(gan.real,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(12000, 28, 28, 1)\n",
      "(72000, 28, 28, 1)\n",
      "(72000,)\n",
      "Train on 57600 samples, validate on 14400 samples\n",
      "57600/57600 [==============================] - 17s 302us/sample - loss: 5.5199e-08 - accuracy: 1.0000 - val_loss: 0.6836 - val_accuracy: 0.1667\n"
     ]
    }
   ],
   "source": [
    "gan.train_discriminator(gan.real, fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
